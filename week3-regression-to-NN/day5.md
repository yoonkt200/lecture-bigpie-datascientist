# 5일차 


-----------------------


#### **1. 분류 알고리즘**


> **1.1 베이즈의 정리**

```
베이즈 정리는 간단하게 말해서, 사전확률을 이용해서 사후확률을 구하는 것이다. 이때, 사전확률은 항상 구해져 있어야 한다. 
만약 사전에 A와 관련된 확률들을 알고 있었다면, A의 확률에 기반하여 새로운 확률변수 B에 대한 확률을 구하는 것이다.
또한, 데이터 내에서의 상대적 중요도를 측정하는 것이라고 할 수 있다.

- http://yamalab.tistory.com/10

> **1.2 나이브 베이즈 분류기**

```
분류하고자 하는 대상의 각 분류별 확률을 측정하여, 확률이 큰 쪽으로 분류하는 것이 베이즈 정리의 요약이다.

나이브 베이즈 분류기는 이러한 베이즈 정리에 기반하여, 알고리즘을 조금 더 naive하게 보는 것이다. 독립변수간의 상관관계등을 고려하지 않았다고 할 수 있다.
```

-----------------------


#### **2. SVM(Support Vector Machine)**

SVM에 대한 대략적인 개념을 먼저 이해할 필요가 있다. SVM은 흔히 서포트 벡터를 분류하는 것을 의미한다. 그러나 분류문제 뿐 아니라
회귀 문제에도 적용이 가능하며 정확도가 매우 높다.

따라서, 이녀석은 일종의 회귀기법으로 각각의 데이터들을 나눌 수 있는 중심선들을 찾는 것이다. 
이걸 통해 중심선을 기준으로 데이터 분류가 가능하다. 즉, 중심선 자체가 하나의 회귀식이 되는 것이다. 

물론 이것은 SVM에 대한 아주 대략적인 설명이고, 
빈도를 기준으로 하느냐, 회귀선을 기준으로 하느냐가 SVM의 종류를 다르게 나타낸다.

중요변수를 찾은 뒤, 중요변수를 기반으로 데이터를 최적으로 나눌 수 있는 직선의 기울기와 절편을 찾겠다는 것.
그러한 직선을 찾는 것이 서포트 벡터 머신이라는 것이다.
만약 직선 근처에 데이터 포인트들이 많으면 에러율이 높다는 거고, 직선 근처에 데이터 포인트가 없도록 하는 것이 최적의 직선이다.
데이터들 사이의 적절한 margin과 직선을 찾는 과정을 반복하게 되는 것이 학습과정이다.

그렇다면 이제, 중요변수를 어떻게 찾을 것인지에 대한 문제가 남았다.
또한 커널함수(변환함수)를 선택하는 것도 문제이다.

핵심 과정 
-> 중요 포인트를 찾고
-> 중요 포인트를 기반으로 직선을 찾고 (초평면이라고도 부름)
-> 이때, 여유변수등을 고려하여 찾는다
-> 고차원 데이터는 비선형을 선형화를 시켜야 하는데, 여기서 커널이라는 개념이 적용된다.

초평면을 찾을 때, 마진을 얼마나 주느냐에 따라 결과가 많이 달라질 수 있음. 
실제로 데이터를 분류하다 보면, 초평면과 +- 마진 사이에 데이터가 물론 들어갈 수 있지만, 이를 최소화하는 게 목표임.
여유변수 -> 어느정도의 이러한 오분류를 허용하겠다는 것.

라그랑스 승수법 : 기존에 나왔던 오차함수에 제약조건을 붙여서 제약조건에 해당하는 값들을 빼주는 방향으로 결과를 유도하는 것.
--> 가중치가 2개가 붙게 된다는 것. --> 이놈을 이용해서 알파값을 알게된다.

여기까지 해서, 필요한 변수는 초평면 직선의 w, b, 그리고 슬랙(?)값(margin?)과 가중치 알파값이 필요해진다.
여기서 각각의 변수에 미분을 해서 요약을 하게 되면 사전에 x,y 를 아는 상태에서 (지도학습이기 때문에) 각각의 변수를 구하는 것이 가능해진다.

여기서 알파값이 제로가 아니면서 마진값 사이에 있다면, 그 알파값을 통해서 중요변수를 구할 수 있게 된다.

즉, 라그랑스를 통해 알파값을 구하고, 알파값을 통해 마진값과 다른 변수들을 연쇄적으로 구할 수 있게 되고, 결국 중요변수를 구할 수 있다.
중요변수를 구하게 되면 이를 통해 초평면을 구할 수 있다.

최종적으로, SVM이라는 알고리즘에서는 알파값만 알게 되면 다른 것들을 최적화해주는 공식을 사용하게 된다

분류기와 회귀머신의 차이는, 만약 입력변수에 sin함수를 취해주면, 기준선에서 위아래를 판단하게 되므로 분류기가 될 수 있다.


벌점화 기법까지 적용하여 강화학습을 수행할 수도 있다.


### 회귀에 대한 추가적 개념

회귀식에 대한 변수로 ax1, bx2, cx3 가 모여 y가 되는 것이 회귀의 원리인데, 만약 결과값이 연속형이라면 회귀가 그대로 되는 것이고

y-y' 가 최소가 되는 방향(비용함수가 최소가 되는 방향)으로 드리블(업데이트) 하는 것이 머신러닝에서의 이슈이다.

이때 모인 애들이 최종적으로 y가 되기 전에 결과값이 [0,1]로 나오도록 다시 변환함수를 한번 거쳐서 하는게 로지스틱이고, 
커널을 이용하는 게 SVM이라고 할 수 있다.

따라서 SVM은 신경망의 일종이라고 할 수 있다.

### 갑자기 뉴럴넷

여기서 변환함수는 활성함수라고도 하는데, 활성함수의 종류에 따라서 알고리즘의 이름이 달라지는 것이다. 예를 들어, 활성함수의 결과값이
이항분류라면 로지스틱이고, 다항분류라면 소프트맥스이다. 

시그모이드 대신에 렐루가 들어갈수도 있는 것.

W는 일단 진행이 되어야 찾을 수 있는데, 그러면 처음에는 찾을 수 없으므로
신경망에서는 W의 초기값을 설정하는게 매우 중요한 문제.

